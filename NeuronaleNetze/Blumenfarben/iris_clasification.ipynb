{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Blumenfarben\n",
    "\n",
    "---\n",
    "\n",
    "## Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Laden Sie das Iris-Datenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Die 3 zu erkennen Klassifikationsklassen werden in die numerischen Werte 0, 1 bzw. 2 umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train.loc[data_train['species']=='Iris-setosa', 'species']=0\n",
    "data_train.loc[data_train['species']=='Iris-versicolor', 'species']=1\n",
    "data_train.loc[data_train['species']=='Iris-virginica', 'species']=2\n",
    "data_train = data_train.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Das eingelesene Datenset wird als Matrix dargestellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train_array = data_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Zur Sicherstellung der Reproduzierbarkeit der Ergebnisse setzen wir random.seed auf einen festen Wert, z. B. 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Trainingsdaten: 80%\n",
    "- Testdaten: 20%\n",
    "- X ist ein Vektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train_array[:, :4],\n",
    "                                                    data_train_array[:, 4],\n",
    "                                                    test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Variante 1:\n",
    "- MultilayerPerceptron\n",
    "- ein Input Layer mit 4 Neuronen für die Merkmale der Pflanzen\n",
    "- ein Hidden-Layer mit 10 Neuronen\n",
    "- ein Output Layer, die die zu erkennenden Klassen repräsentieren\n",
    "- Aktivierungsfunktion: relu\n",
    "- Optimierer: adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=400, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Variante 2\n",
    "- 2 Hiddenlayer mit 5 bzw. 3 Neuronen\n",
    "- Aktivierungsfunktion: tahn\n",
    "- Optimierer: adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(5,3), activation='tanh', solver='adam', max_iter=350, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23714356\n",
      "Iteration 2, loss = 1.16500287\n",
      "Iteration 3, loss = 1.09078111\n",
      "Iteration 4, loss = 1.01172810\n",
      "Iteration 5, loss = 0.93696345\n",
      "Iteration 6, loss = 0.87780471\n",
      "Iteration 7, loss = 0.83544387\n",
      "Iteration 8, loss = 0.80997823\n",
      "Iteration 9, loss = 0.78593259\n",
      "Iteration 10, loss = 0.75938272\n",
      "Iteration 11, loss = 0.73223921\n",
      "Iteration 12, loss = 0.70655381\n",
      "Iteration 13, loss = 0.68128318\n",
      "Iteration 14, loss = 0.65962107\n",
      "Iteration 15, loss = 0.63910613\n",
      "Iteration 16, loss = 0.62142243\n",
      "Iteration 17, loss = 0.60459964\n",
      "Iteration 18, loss = 0.58800212\n",
      "Iteration 19, loss = 0.57209376\n",
      "Iteration 20, loss = 0.55784745\n",
      "Iteration 21, loss = 0.54543424\n",
      "Iteration 22, loss = 0.53434454\n",
      "Iteration 23, loss = 0.52573065\n",
      "Iteration 24, loss = 0.51554439\n",
      "Iteration 25, loss = 0.50562854\n",
      "Iteration 26, loss = 0.49668817\n",
      "Iteration 27, loss = 0.48530308\n",
      "Iteration 28, loss = 0.46950774\n",
      "Iteration 29, loss = 0.45237203\n",
      "Iteration 30, loss = 0.43890222\n",
      "Iteration 31, loss = 0.42945291\n",
      "Iteration 32, loss = 0.42288212\n",
      "Iteration 33, loss = 0.41486142\n",
      "Iteration 34, loss = 0.40741484\n",
      "Iteration 35, loss = 0.40083442\n",
      "Iteration 36, loss = 0.39383447\n",
      "Iteration 37, loss = 0.38757941\n",
      "Iteration 38, loss = 0.38181347\n",
      "Iteration 39, loss = 0.37544937\n",
      "Iteration 40, loss = 0.36706921\n",
      "Iteration 41, loss = 0.36142126\n",
      "Iteration 42, loss = 0.35480901\n",
      "Iteration 43, loss = 0.34928774\n",
      "Iteration 44, loss = 0.34255668\n",
      "Iteration 45, loss = 0.33661181\n",
      "Iteration 46, loss = 0.33025675\n",
      "Iteration 47, loss = 0.32545966\n",
      "Iteration 48, loss = 0.32005975\n",
      "Iteration 49, loss = 0.31360648\n",
      "Iteration 50, loss = 0.30763004\n",
      "Iteration 51, loss = 0.30278963\n",
      "Iteration 52, loss = 0.29786755\n",
      "Iteration 53, loss = 0.29183938\n",
      "Iteration 54, loss = 0.28868129\n",
      "Iteration 55, loss = 0.28558300\n",
      "Iteration 56, loss = 0.27838185\n",
      "Iteration 57, loss = 0.27279635\n",
      "Iteration 58, loss = 0.26789686\n",
      "Iteration 59, loss = 0.26309474\n",
      "Iteration 60, loss = 0.25957706\n",
      "Iteration 61, loss = 0.25586697\n",
      "Iteration 62, loss = 0.24892036\n",
      "Iteration 63, loss = 0.24697793\n",
      "Iteration 64, loss = 0.24119477\n",
      "Iteration 65, loss = 0.23803299\n",
      "Iteration 66, loss = 0.23359880\n",
      "Iteration 67, loss = 0.23003975\n",
      "Iteration 68, loss = 0.22584638\n",
      "Iteration 69, loss = 0.22415230\n",
      "Iteration 70, loss = 0.21853226\n",
      "Iteration 71, loss = 0.21521102\n",
      "Iteration 72, loss = 0.21262995\n",
      "Iteration 73, loss = 0.20888092\n",
      "Iteration 74, loss = 0.20595279\n",
      "Iteration 75, loss = 0.20286860\n",
      "Iteration 76, loss = 0.20032217\n",
      "Iteration 77, loss = 0.19777138\n",
      "Iteration 78, loss = 0.19328472\n",
      "Iteration 79, loss = 0.19130542\n",
      "Iteration 80, loss = 0.18835166\n",
      "Iteration 81, loss = 0.18506342\n",
      "Iteration 82, loss = 0.18232866\n",
      "Iteration 83, loss = 0.18016901\n",
      "Iteration 84, loss = 0.17757688\n",
      "Iteration 85, loss = 0.17529715\n",
      "Iteration 86, loss = 0.17330065\n",
      "Iteration 87, loss = 0.17120536\n",
      "Iteration 88, loss = 0.16735877\n",
      "Iteration 89, loss = 0.16593536\n",
      "Iteration 90, loss = 0.16415349\n",
      "Iteration 91, loss = 0.16156247\n",
      "Iteration 92, loss = 0.15902494\n",
      "Iteration 93, loss = 0.15834523\n",
      "Iteration 94, loss = 0.15593047\n",
      "Iteration 95, loss = 0.15327733\n",
      "Iteration 96, loss = 0.15167721\n",
      "Iteration 97, loss = 0.15186080\n",
      "Iteration 98, loss = 0.14782991\n",
      "Iteration 99, loss = 0.14642645\n",
      "Iteration 100, loss = 0.14521507\n",
      "Iteration 101, loss = 0.14406250\n",
      "Iteration 102, loss = 0.14117515\n",
      "Iteration 103, loss = 0.14054108\n",
      "Iteration 104, loss = 0.13761322\n",
      "Iteration 105, loss = 0.13672783\n",
      "Iteration 106, loss = 0.13708513\n",
      "Iteration 107, loss = 0.13467442\n",
      "Iteration 108, loss = 0.13260052\n",
      "Iteration 109, loss = 0.13126268\n",
      "Iteration 110, loss = 0.12963110\n",
      "Iteration 111, loss = 0.12989761\n",
      "Iteration 112, loss = 0.12870003\n",
      "Iteration 113, loss = 0.12638524\n",
      "Iteration 114, loss = 0.12674865\n",
      "Iteration 115, loss = 0.12270858\n",
      "Iteration 116, loss = 0.12204885\n",
      "Iteration 117, loss = 0.12060770\n",
      "Iteration 118, loss = 0.11960100\n",
      "Iteration 119, loss = 0.11823395\n",
      "Iteration 120, loss = 0.11745101\n",
      "Iteration 121, loss = 0.11552883\n",
      "Iteration 122, loss = 0.11500302\n",
      "Iteration 123, loss = 0.11510561\n",
      "Iteration 124, loss = 0.11426505\n",
      "Iteration 125, loss = 0.11204739\n",
      "Iteration 126, loss = 0.11207547\n",
      "Iteration 127, loss = 0.11116119\n",
      "Iteration 128, loss = 0.11238799\n",
      "Iteration 129, loss = 0.11220901\n",
      "Iteration 130, loss = 0.10829233\n",
      "Iteration 131, loss = 0.10612814\n",
      "Iteration 132, loss = 0.10635068\n",
      "Iteration 133, loss = 0.10412866\n",
      "Iteration 134, loss = 0.10464552\n",
      "Iteration 135, loss = 0.10283410\n",
      "Iteration 136, loss = 0.10492819\n",
      "Iteration 137, loss = 0.10253084\n",
      "Iteration 138, loss = 0.10132884\n",
      "Iteration 139, loss = 0.09965193\n",
      "Iteration 140, loss = 0.09939448\n",
      "Iteration 141, loss = 0.10108345\n",
      "Iteration 142, loss = 0.09733341\n",
      "Iteration 143, loss = 0.09844709\n",
      "Iteration 144, loss = 0.09580282\n",
      "Iteration 145, loss = 0.09641836\n",
      "Iteration 146, loss = 0.09781817\n",
      "Iteration 147, loss = 0.09335216\n",
      "Iteration 148, loss = 0.09407244\n",
      "Iteration 149, loss = 0.09370285\n",
      "Iteration 150, loss = 0.09327025\n",
      "Iteration 151, loss = 0.09185278\n",
      "Iteration 152, loss = 0.09066582\n",
      "Iteration 153, loss = 0.09080535\n",
      "Iteration 154, loss = 0.08960653\n",
      "Iteration 155, loss = 0.08936754\n",
      "Iteration 156, loss = 0.08843240\n",
      "Iteration 157, loss = 0.08713256\n",
      "Iteration 158, loss = 0.08744166\n",
      "Iteration 159, loss = 0.08646104\n",
      "Iteration 160, loss = 0.08561434\n",
      "Iteration 161, loss = 0.08645064\n",
      "Iteration 162, loss = 0.08655636\n",
      "Iteration 163, loss = 0.08826439\n",
      "Iteration 164, loss = 0.08431293\n",
      "Iteration 165, loss = 0.08468612\n",
      "Iteration 166, loss = 0.08658569\n",
      "Iteration 167, loss = 0.08577731\n",
      "Iteration 168, loss = 0.08678000\n",
      "Iteration 169, loss = 0.08304603\n",
      "Iteration 170, loss = 0.08142972\n",
      "Iteration 171, loss = 0.08064751\n",
      "Iteration 172, loss = 0.08497296\n",
      "Iteration 173, loss = 0.07945134\n",
      "Iteration 174, loss = 0.07943137\n",
      "Iteration 175, loss = 0.07914853\n",
      "Iteration 176, loss = 0.07784693\n",
      "Iteration 177, loss = 0.08017148\n",
      "Iteration 178, loss = 0.07780847\n",
      "Iteration 179, loss = 0.07917035\n",
      "Iteration 180, loss = 0.07955009\n",
      "Iteration 181, loss = 0.07684512\n",
      "Iteration 182, loss = 0.07698830\n",
      "Iteration 183, loss = 0.07593812\n",
      "Iteration 184, loss = 0.08033598\n",
      "Iteration 185, loss = 0.08173443\n",
      "Iteration 186, loss = 0.07850563\n",
      "Iteration 187, loss = 0.07611691\n",
      "Iteration 188, loss = 0.07478173\n",
      "Iteration 189, loss = 0.07729936\n",
      "Iteration 190, loss = 0.07318065\n",
      "Iteration 191, loss = 0.07351339\n",
      "Iteration 192, loss = 0.07217551\n",
      "Iteration 193, loss = 0.07264476\n",
      "Iteration 194, loss = 0.07236919\n",
      "Iteration 195, loss = 0.07240629\n",
      "Iteration 196, loss = 0.07615456\n",
      "Iteration 197, loss = 0.07059983\n",
      "Iteration 198, loss = 0.07167898\n",
      "Iteration 199, loss = 0.07080774\n",
      "Iteration 200, loss = 0.07075641\n",
      "Iteration 201, loss = 0.07126183\n",
      "Iteration 202, loss = 0.07031962\n",
      "Iteration 203, loss = 0.06910579\n",
      "Iteration 204, loss = 0.07284497\n",
      "Iteration 205, loss = 0.07011601\n",
      "Iteration 206, loss = 0.07261928\n",
      "Iteration 207, loss = 0.06879266\n",
      "Iteration 208, loss = 0.06756268\n",
      "Iteration 209, loss = 0.06786061\n",
      "Iteration 210, loss = 0.07129676\n",
      "Iteration 211, loss = 0.06993940\n",
      "Iteration 212, loss = 0.06650871\n",
      "Iteration 213, loss = 0.06668129\n",
      "Iteration 214, loss = 0.06779044\n",
      "Iteration 215, loss = 0.06584484\n",
      "Iteration 216, loss = 0.06501114\n",
      "Iteration 217, loss = 0.06607690\n",
      "Iteration 218, loss = 0.06642980\n",
      "Iteration 219, loss = 0.06636753\n",
      "Iteration 220, loss = 0.06464979\n",
      "Iteration 221, loss = 0.06535342\n",
      "Iteration 222, loss = 0.06410427\n",
      "Iteration 223, loss = 0.06499543\n",
      "Iteration 224, loss = 0.06444345\n",
      "Iteration 225, loss = 0.06548699\n",
      "Iteration 226, loss = 0.06752154\n",
      "Iteration 227, loss = 0.06299254\n",
      "Iteration 228, loss = 0.06323032\n",
      "Iteration 229, loss = 0.06757865\n",
      "Iteration 230, loss = 0.06519721\n",
      "Iteration 231, loss = 0.06370473\n",
      "Iteration 232, loss = 0.06302419\n",
      "Iteration 233, loss = 0.06156424\n",
      "Iteration 234, loss = 0.06074301\n",
      "Iteration 235, loss = 0.06263623\n",
      "Iteration 236, loss = 0.06878256\n",
      "Iteration 237, loss = 0.06623165\n",
      "Iteration 238, loss = 0.06364597\n",
      "Iteration 239, loss = 0.06128597\n",
      "Iteration 240, loss = 0.06167802\n",
      "Iteration 241, loss = 0.06109506\n",
      "Iteration 242, loss = 0.06036151\n",
      "Iteration 243, loss = 0.06041420\n",
      "Iteration 244, loss = 0.05950490\n",
      "Iteration 245, loss = 0.06269027\n",
      "Iteration 246, loss = 0.05945444\n",
      "Iteration 247, loss = 0.05898882\n",
      "Iteration 248, loss = 0.06221246\n",
      "Iteration 249, loss = 0.05895207\n",
      "Iteration 250, loss = 0.05949515\n",
      "Iteration 251, loss = 0.05806711\n",
      "Iteration 252, loss = 0.06106907\n",
      "Iteration 253, loss = 0.05708663\n",
      "Iteration 254, loss = 0.05813307\n",
      "Iteration 255, loss = 0.05768597\n",
      "Iteration 256, loss = 0.05758359\n",
      "Iteration 257, loss = 0.05776434\n",
      "Iteration 258, loss = 0.05764756\n",
      "Iteration 259, loss = 0.05730639\n",
      "Iteration 260, loss = 0.05708798\n",
      "Iteration 261, loss = 0.05710694\n",
      "Iteration 262, loss = 0.05694094\n",
      "Iteration 263, loss = 0.05573400\n",
      "Iteration 264, loss = 0.05590490\n",
      "Iteration 265, loss = 0.05658653\n",
      "Iteration 266, loss = 0.05673266\n",
      "Iteration 267, loss = 0.05719762\n",
      "Iteration 268, loss = 0.05503046\n",
      "Iteration 269, loss = 0.05668287\n",
      "Iteration 270, loss = 0.05526478\n",
      "Iteration 271, loss = 0.05656013\n",
      "Iteration 272, loss = 0.05495085\n",
      "Iteration 273, loss = 0.06152096\n",
      "Iteration 274, loss = 0.05439663\n",
      "Iteration 275, loss = 0.05578463\n",
      "Iteration 276, loss = 0.05510031\n",
      "Iteration 277, loss = 0.05535855\n",
      "Iteration 278, loss = 0.05510821\n",
      "Iteration 279, loss = 0.05417913\n",
      "Iteration 280, loss = 0.05362060\n",
      "Iteration 281, loss = 0.05696269\n",
      "Iteration 282, loss = 0.05960035\n",
      "Iteration 283, loss = 0.05312537\n",
      "Iteration 284, loss = 0.05407491\n",
      "Iteration 285, loss = 0.05333392\n",
      "Iteration 286, loss = 0.05422925\n",
      "Iteration 287, loss = 0.05440600\n",
      "Iteration 288, loss = 0.05858199\n",
      "Iteration 289, loss = 0.05737805\n",
      "Iteration 290, loss = 0.05389949\n",
      "Iteration 291, loss = 0.06194759\n",
      "Iteration 292, loss = 0.04890715\n",
      "Iteration 293, loss = 0.05436943\n",
      "Iteration 294, loss = 0.05517095\n",
      "Iteration 295, loss = 0.04962803\n",
      "Iteration 296, loss = 0.05742422\n",
      "Iteration 297, loss = 0.05399980\n",
      "Iteration 298, loss = 0.05246274\n",
      "Iteration 299, loss = 0.05555857\n",
      "Iteration 300, loss = 0.05207879\n",
      "Iteration 301, loss = 0.05298915\n",
      "Iteration 302, loss = 0.05302963\n",
      "Iteration 303, loss = 0.05270617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, batch_size=10, hidden_layer_sizes=(5, 3),\n",
       "              max_iter=350, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, batch_size=10, hidden_layer_sizes=(5, 3),\n",
       "              max_iter=350, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', batch_size=10, hidden_layer_sizes=(5, 3),\n",
       "              max_iter=350, verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsergebnis: 0.992\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainingsergebnis: %5.3f\" % mlp.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Das Modell mit den Testdaten evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## die Konfusionsmatrix wird ausgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Score berechnen und ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         7\n",
      "         1.0       0.92      1.00      0.96        11\n",
      "         2.0       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testen des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testergebnis: 0.967\n"
     ]
    }
   ],
   "source": [
    "print(\"Testergebnis: %5.3f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gewichtung ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [array([[-0.18175367,  0.51967945, -0.03682395, -0.1205498 ,  0.16938574],\n",
      "       [-0.71339316,  0.1364212 ,  0.68704501,  0.83270091,  0.40263018],\n",
      "       [ 1.02301099, -0.62040153, -0.54177161, -0.58501333, -0.41696715],\n",
      "       [ 0.53283474, -0.92627887,  0.24828775,  1.00029128, -0.58660593]]), array([[-1.64319389,  0.99261637,  0.48133661],\n",
      "       [ 0.19205787, -0.35979584,  1.30116347],\n",
      "       [ 0.49705423, -0.08875001,  1.10067155],\n",
      "       [ 0.31704068, -1.41095649, -0.4765751 ],\n",
      "       [ 0.53497596, -0.47725283,  2.05482876]]), array([[ 1.80286371, -0.92012959, -0.82321987],\n",
      "       [-2.25024998,  0.78176397,  0.15917304],\n",
      "       [ 0.98562046,  2.12981661, -2.50680455]])]\n",
      "BIASES: [array([ 0.36436877,  0.79437877,  0.03455584, -0.69635934,  0.69751936]), array([-0.32413316,  0.70181907,  0.44662492]), array([ 0.51175497, -0.31347659,  0.17286358])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights:\", mlp.coefs_)\n",
    "print(\"BIASES:\", mlp.intercepts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vorhersage\n",
    "Das Modell wird beispielsweise zur Vorhersage auf folgenden Werten aus dem Testset angewandt mit den Merkmalen [sepal-length, sepal-width, petal-length, petal-width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(mlp.predict([[5.1,3.5,1.4,0.2], [5.9,3.,5.1,1.8], [4.9,3.,1.4,0.2], [5.8,2.7,4.1,1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = mlp.loss_curve_\n",
    "plt.plot(loss_values)\n",
    "plt.savefig(\"./Plot_of_loss_values.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
